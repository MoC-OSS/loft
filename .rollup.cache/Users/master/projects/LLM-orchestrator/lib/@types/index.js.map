{"version":3,"file":"index.js","sourceRoot":"","sources":["../../src/@types/index.ts"],"names":[],"mappings":"AA0CA,MAAM,CAAN,IAAY,2BAKX;AALD,WAAY,2BAA2B;IACrC,sDAAuB,CAAA;IACvB,sDAAuB,CAAA;IACvB,wDAAyB,CAAA;IACzB,0EAA2C,CAAA;AAC7C,CAAC,EALW,2BAA2B,KAA3B,2BAA2B,QAKtC;AAyCD,MAAM,CAAN,IAAY,gBAKX;AALD,WAAY,gBAAgB;IAC1B,yCAAqB,CAAA;IACrB,6CAAyB,CAAA;IACzB,iDAA6B,CAAA;IAC7B,iCAAa,CAAA;AACf,CAAC,EALW,gBAAgB,KAAhB,gBAAgB,QAK3B","sourcesContent":["import { SystemMessageType } from '../schema/CreateChatCompletionRequestSchema';\nimport { PromptType } from '../schema/PromptSchema';\nimport { Session } from '../session/Session';\nimport { ChatHistory } from '../session/ChatHistory';\nimport { Message } from '../session/Message';\n\nexport type PalmExample = {\n  input: { content: string };\n  output: { content: string };\n};\n\nexport interface SessionProps {\n  sessionId: string;\n  systemMessageName: string;\n  systemMessage: string;\n  modelPreset: SystemMessageType['modelPreset'];\n  messages: ChatHistory;\n  examples: PalmExample[];\n  model: string;\n  lastMessageByRole: {\n    user: Message | null;\n    assistant: Message | null;\n  };\n  handlersCount: Record<string, number>;\n  ctx: Record<string, unknown>;\n  messageAccumulator: Message[] | null;\n  createdAt: number;\n  updatedAt: number;\n  lastError: string | null;\n}\nexport interface InputPayload {\n  sessionId: string;\n  systemMessageName: string;\n  message: string;\n}\n\nexport interface ChatInputPayload {\n  sessionId: string;\n  systemMessageName: string;\n  messages: Message[];\n}\n\nexport enum ChatCompletionCallInitiator {\n  main_flow = 'MAIN_FLOW',\n  injection = 'INJECTION',\n  call_again = 'CALL_AGAIN',\n  set_function_result = 'SET_FUNCTION_RESULT',\n}\n\nexport interface OutputContext {\n  initiator: ChatCompletionCallInitiator;\n  session: Session;\n  llmResponse?: PredictionResponse['predictions'][number];\n}\n\nexport type IOContext = ChatInputPayload | OutputContext;\n\nexport interface Config {\n  nodeEnv: string;\n  appName: string;\n  redisHost: string;\n  redisPort: number;\n  bullMqDb: number;\n  llmRateLimiter: {\n    /**\n     * Max number of jobs to process in the time period\n     * specified in `duration`.\n     */\n    max: number;\n    /**\n     * Time in milliseconds. During this time, a maximum\n     * of `max` jobs will be processed.\n     */\n    duration: number;\n    /**\n     * Amount of jobs that a single worker is allowed to work on\n     * in parallel.\n     *\n     * @default 1\n     * @see {@link https://docs.bullmq.io/guide/workers/concurrency}\n     */\n    concurrency: number;\n  };\n  jobsLockDuration: number; // in milliseconds\n  jobsAttempts: number;\n  chatCompletionJobCallAttempts: number;\n}\n\nexport enum MiddlewareStatus {\n  CONTINUE = 'CONTINUE',\n  CALL_AGAIN = 'CALL_AGAIN',\n  NOT_RETURNED = 'NOT_RETURNED',\n  STOP = 'STOP',\n}\n\nexport type AsyncLLMInputMiddleware = (\n  context: ChatInputPayload,\n  next: (input: ChatInputPayload) => Promise<void>,\n) => Promise<void>;\n\nexport type AsyncLLMOutputMiddleware = (\n  context: OutputContext,\n  next: (output: OutputContext) => Promise<void>,\n) => Promise<{\n  status?: MiddlewareStatus;\n  newOutputContext: OutputContext | undefined;\n}>;\n\nexport type LLMInputMiddlewares = Map<string, AsyncLLMInputMiddleware>;\nexport type LLMOutputMiddlewares = Map<string, AsyncLLMOutputMiddleware>;\n\nexport type SystemMessageComputer = (\n  input: SystemMessageType,\n  context: ChatInputPayload,\n) => Promise<SystemMessageType>;\n\nexport type SystemMessageComputers = Map<string, SystemMessageComputer>;\n\nexport type PromptComputer = (\n  input: PromptType,\n  context: Session,\n) => Promise<PromptType>;\n\nexport type PromptComputers = Map<string, PromptComputer>;\n\nexport type ErrorProperties =\n  | Partial<OutputContext>\n  | {\n      initiator: ChatCompletionCallInitiator;\n      sessionId: Session['sessionId'];\n      systemMessageName: Session['systemMessageName'];\n      messages: Message[];\n    }\n  | undefined;\n\nexport type ErrorHandler = (\n  error: Error | unknown,\n  response?: ErrorProperties,\n) => Promise<void>;\n\n// ========================================================================\nimport { protos } from '@google-ai/generativelanguage';\nimport { PredictionResponse } from '../llm/Palm/@types/response';\n\nexport type IGenerateMessageResponse =\n  protos.google.ai.generativelanguage.v1beta2.IGenerateMessageResponse;\nexport type IGenerateMessageRequest =\n  protos.google.ai.generativelanguage.v1beta2.IGenerateMessageRequest;\n\nexport type IMessagePrompt =\n  protos.google.ai.generativelanguage.v1beta2.IMessagePrompt;\nexport type PalmMessage = protos.google.ai.generativelanguage.v1beta2.IMessage;\n"]}